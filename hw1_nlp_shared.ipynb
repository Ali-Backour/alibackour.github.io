{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Backour/alibackour.github.io/blob/main/hw1_nlp_shared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD98gW5NbOVA",
        "outputId": "1dd2362b-3580-4d58-aa65-db91f1fc94c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHomework 1: Language Models\\n\\nThis assignment will guide you through implementing three different types of language models:\\n1. N-gram model\\n2. Log-linear model with hand-designed features\\n3. Continuous Bag of Words (CBOW) model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Homework 1: Language Models\n",
        "\n",
        "This assignment will guide you through implementing three different types of language models:\n",
        "1. N-gram model\n",
        "2. Log-linear model with hand-designed features\n",
        "3. Continuous Bag of Words (CBOW) model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Optional, Tuple\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.models import BPE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "a5ooSqa9bTXF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel:\n",
        "    \"\"\"\n",
        "    Abstract base class for language models.\n",
        "\n",
        "    This class defines the interface that all language models should implement.\n",
        "    Language models take a sequence of tokens (represented as integers) and\n",
        "    predict probability distributions over the next token in the vocabulary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.eos = 'eos'\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the language model on a collection of token sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences, where each\n",
        "                                               sequence is a numpy array of integers\n",
        "                                               representing token IDs.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the train method\")\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens given a context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Array of token IDs representing the context.\n",
        "                                 Shape: (context_length,)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary.\n",
        "                       Shape: (vocab_size,)\n",
        "                       Should sum to 1.0.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement the get_next_token_probs method\")\n",
        "\n",
        "    def perplexity(self, token_sequences: List[np.ndarray]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate perplexity of the model on a set of token sequences.\n",
        "\n",
        "        Perplexity is 2^(-average_log_likelihood), where average log likelihood\n",
        "        is calculated over all tokens in all sequences.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): List of token sequences to evaluate\n",
        "\n",
        "        Returns:\n",
        "            float: Perplexity score (lower is better)\n",
        "        \"\"\"\n",
        "        total_log_likelihood = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for sequence in token_sequences:\n",
        "            if len(sequence) < 2:\n",
        "                continue  # Need at least 2 tokens (context + target)\n",
        "\n",
        "            for i in range(1, len(sequence)):\n",
        "                context = sequence[:i]\n",
        "                target_token = sequence[i]\n",
        "\n",
        "                probs = self.get_next_token_probs(context)\n",
        "                # Add small epsilon to avoid log(0)\n",
        "                prob = max(float(probs[target_token]), 1e-10)\n",
        "                total_log_likelihood += np.log2(prob)\n",
        "                total_tokens += 1\n",
        "\n",
        "        if total_tokens == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        average_log_likelihood = total_log_likelihood / total_tokens\n",
        "        return 2 ** (-average_log_likelihood)\n",
        "\n",
        "    def generate_text(self, context: np.ndarray, max_length: int = 100,\n",
        "                     temperature: float = 1.0) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generate text by sampling from the model.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Initial context tokens\n",
        "            max_length (int): Maximum number of tokens to generate\n",
        "            temperature (float): Sampling temperature (higher = more random)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Generated sequence including the initial context\n",
        "        \"\"\"\n",
        "        generated = list(context)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            current_context = np.array(generated)\n",
        "            probs = self.get_next_token_probs(current_context)\n",
        "\n",
        "            # Apply temperature\n",
        "            if temperature != 1.0:\n",
        "                probs = np.power(probs, 1.0 / temperature)\n",
        "                probs = probs / np.sum(probs)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token = np.random.choice(len(probs), p=probs)\n",
        "            generated.append(next_token)\n",
        "\n",
        "            # Optional: add stopping criteria here (e.g., end-of-sequence token)\n",
        "            if next_token == self.eos:\n",
        "              break\n",
        "\n",
        "        return np.array(generated)"
      ],
      "metadata": {
        "id": "Ywr2egPPbVQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    N-gram language model using maximum likelihood estimation with smoothing.\n",
        "\n",
        "    This model predicts the next token based on the previous n-1 tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, n: int = 3, smoothing=None):\n",
        "        \"\"\"\n",
        "        Initialize the N-gram model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            n (int): Order of the n-gram (default: 3 for trigram)\n",
        "\n",
        "            Bonus! Without smoothing your perplexity will be pretty bad. If you\n",
        "            implement some kind of smoothing and get the perplexity below 300\n",
        "            you'll get extra credit.\n",
        "            smoothing (str): Smoothing method ('laplace' or 'interpolation')\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Consider using nested dictionaries or defaultdict(Counter) to store counts.\n",
        "        # Hint: Consider how you will handle different context lengths.\n",
        "        # At the start of a sentence, you might have 0, 1, or 2 words of context\n",
        "        # instead of the full n-1 words.\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n = n\n",
        "        self.counts = {i:{} for i in range(1, n+1)}\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray]) -> None:\n",
        "        \"\"\"\n",
        "        Train the n-gram model by counting n-grams in the training data.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution over next tokens for given context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: Try contexts from longest to shortest, i.e., try the full context,\n",
        "        # and if it is not in the training data, try shorter context\n",
        "        # Hint: What probability distribution should we output if no valid context is found?"
      ],
      "metadata": {
        "id": "qqLcUlo4bY4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogLinearModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Log-linear language model with hand-designed features.\n",
        "\n",
        "    This model uses a linear combination of features to predict next token probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, context_size: int = 3):\n",
        "        \"\"\"\n",
        "        Initialize the log-linear model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            context_size (int): Number of context tokens to consider\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "        # Hint: You may find the class nn.Linear useful\n",
        "        # If this is too slow or using too much memory, check out the nn.EmbeddingBag class\n",
        "        # and see if that's applicable to your use case\n",
        "        self.linear = None # TODO: replace me!\n",
        "        # What loss function should we use?\n",
        "        self.criterion = None # TODO: replace me!\n",
        "        # We use an Adam optimizer. This is a fancy version of SGD which uses momentum and adaptive updates.\n",
        "        self.optimizer = optim.Adam(\"TODO: replace me!\", lr=0.01)\n",
        "\n",
        "\n",
        "    def extract_features(self, context: np.ndarray) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extract features from the context.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Feature vector produced from context tokens\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 2, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the log-linear model using gradient descent.\n",
        "        \"\"\"\n",
        "        # Create training examples (context, target) pairs\n",
        "        contexts_list = []\n",
        "        targets_list = []\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "\n",
        "        print(f\"Training on {len(all_features)} examples for {epochs} epochs...\")\n",
        "\n",
        "        # Training loop\n",
        "\n",
        "        # TODO: Put your layers in training mode\n",
        "\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Mini-batch training\n",
        "            # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "            for i in tqdm(range(0, len(all_features), batch_size)):\n",
        "                batch_contexts= contexts_list[i:i+batch_size]\n",
        "                batch_targets = targets_list[i:i+batch_size]\n",
        "\n",
        "                # TODO: Get features for the batch\n",
        "\n",
        "\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                # Your code here!\n",
        "\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                # Your code here!\n",
        "\n",
        "\n",
        "                # TODO: Perform the backward pass and gradient update. Remember,\n",
        "                # you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = None # TODO: calculate me!\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get probability distribution using softmax over linear scores.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hint: What probability distribution should we output if no valid context is found?"
      ],
      "metadata": {
        "id": "Celb4A0sbkAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWModel(LanguageModel):\n",
        "    \"\"\"\n",
        "    Continuous Bag of Words (CBOW) model.\n",
        "\n",
        "    This model learns dense vector representations of words and predicts\n",
        "    the next word from the context words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int = 100, context_size: int = 2, learning_rate: float = 0.01):\n",
        "        \"\"\"\n",
        "        Initialize the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            context_size (int): Number of context words on each side\n",
        "            learning_rate (float): Learning rate for training\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # You may find the classes nn.Embedding and nn.EmbeddingBag useful\n",
        "\n",
        "\n",
        "        # We use an Adam optimizer. This is a fancy version of SGD which uses momentum and adaptive updates.\n",
        "        self.optimizer = optim.Adam(\"TODO: replace me!\", lr=learning_rate)\n",
        "\n",
        "        # What loss function should we use for Word2Vec?\n",
        "        self.criterion = None # TODO: replace me!\n",
        "\n",
        "\n",
        "    def train(self, token_sequences: List[np.ndarray], epochs: int = 10, batch_size: int = 32) -> None:\n",
        "        \"\"\"\n",
        "        Train the CBOW model.\n",
        "\n",
        "        Args:\n",
        "            token_sequences (List[np.ndarray]): Training sequences\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Create training examples (context, target) pairs\n",
        "        # Hint, extract left context only for next token prediction\n",
        "        # Hint, pad shorter contexts with 0, this ensures all have the same length for batching\n",
        "        context_list = []\n",
        "        targets_list = []\n",
        "\n",
        "        # TODO: Put your layers in training mode\n",
        "\n",
        "        losses = []  # Potentially useful for debugging (loss should go down!)\n",
        "        # Note: tqdm is used to display progress bars for loops, helping visualize training progress.\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            total_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            # Shuffle data\n",
        "            indices = torch.randperm(len(all_contexts))\n",
        "            all_contexts = all_contexts[indices]\n",
        "            all_targets = all_targets[indices]\n",
        "\n",
        "            for i in tqdm(range(0, len(all_contexts), batch_size)):\n",
        "                # As an alternative to this implementation, you can experiment with\n",
        "                # DataLoader (https://docs.pytorch.org/docs/stable/data.html) for automatic shuffling, parallel loading\n",
        "                batch_contexts = all_contexts[i:i+batch_size]\n",
        "                batch_targets = all_targets[i:i+batch_size]\n",
        "\n",
        "                # TODO: Zero the gradients of the optimizer\n",
        "                # Your code here!\n",
        "\n",
        "                # TODO: Perform a forward pass to compute predictions for the model.\n",
        "                # Your code here!\n",
        "                preds = None\n",
        "\n",
        "                # TODO: Finish the backward pass and gradient update.\n",
        "                # Remember, you need to compute the loss, perform the backward pass, and\n",
        "                # update the model parameters.\n",
        "                # Your code here!\n",
        "                loss = None # TODO: calculate me!\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "                if i % (batch_size * 10) == 0:  # Print every 10 batches\n",
        "                    print(f\"Epoch {epoch}, Batch {i // batch_size}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            if epoch % 2 == 0:\n",
        "                avg_loss = total_loss / num_batches\n",
        "                print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            losses.append(total_loss)\n",
        "\n",
        "    def get_next_token_probs(self, context: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get next-token probability distributions.\n",
        "\n",
        "        Args:\n",
        "            context (np.ndarray): Context tokens\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Probability distribution over vocabulary\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "        # Hints:\n",
        "        # For next-token prediction, we use the last context_size tokens\n",
        "        # No valid context, return uniform distribution\n",
        "        # Pad context to expected size\n",
        "        # Don't forget to add batch dimension, torch expects (batch_size, context_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pass\n",
        "\n",
        "    def get_word_embedding(self, token_id: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get the learned embedding for a specific token.\n",
        "\n",
        "        Args:\n",
        "            token_id (int): Token ID\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Word embedding vector\n",
        "        \"\"\"\n",
        "        # TODO: YOUR CODE HERE\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pass"
      ],
      "metadata": {
        "id": "WT1D2snWcHSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filepath: str, tokenizer: Optional[Tokenizer], max_seq_length: int = 512) -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load and preprocess text data using GPT-2 tokenizer.\n",
        "\n",
        "    This function is provided complete - students don't need to modify it.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the text file\n",
        "        tokenizer (Optional[Tokenizer]): Tokenizer to use. If None, a new tokenizer will be created.\n",
        "        max_seq_length (int): Maximum sequence length for splitting text\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[np.ndarray], GPT2Tokenizer]: List of token sequences and the tokenizer\n",
        "    \"\"\"\n",
        "    # Byte Pair Encoding (BPE)\n",
        "\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(BPE())\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        #trainer = BpeTrainer(special_tokens=[\"[PAD]\"])\n",
        "        tokenizer.train([filepath])\n",
        "\n",
        "    # Read the text file\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenize the entire text\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "\n",
        "    # Split into sequences of max_seq_length\n",
        "    sequences = []\n",
        "    for i in range(0, len(tokens), max_seq_length):\n",
        "        sequence = tokens[i:i+max_seq_length]\n",
        "        if len(sequence) > 1:  # Need at least 2 tokens for language modeling\n",
        "            sequences.append(np.array(sequence))\n",
        "\n",
        "    print(f\"Loaded {len(sequences)} sequences from {filepath}\")\n",
        "    print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
        "    print(f\"Sample tokens: {tokens[:10]}\")\n",
        "    print(f\"Sample text: {tokenizer.decode(tokens[:10])}\")\n",
        "\n",
        "    return sequences, tokenizer"
      ],
      "metadata": {
        "id": "_3fWkVnxcXu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(models: List[LanguageModel], test_data: List[np.ndarray]) -> None:\n",
        "    \"\"\"\n",
        "    Evaluate and compare multiple language models.\n",
        "\n",
        "    Args:\n",
        "        models (List[LanguageModel]): List of trained models\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "    \"\"\"\n",
        "    print(\"Model Evaluation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        model_name = model.__class__.__name__\n",
        "        try:\n",
        "            ppl = model.perplexity(test_data)\n",
        "            print(f\"{model_name}: Perplexity = {ppl:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name}: Error calculating perplexity - {e}\")\n",
        "\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "0ItN7U7ocbkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze(model1: LanguageModel, model2: LanguageModel, test_data: List[np.ndarray],\n",
        "           tokenizer=None, context_length: int = 2) -> dict:\n",
        "    \"\"\"\n",
        "    Compare two models and find contexts where each performs better.\n",
        "\n",
        "    Args:\n",
        "        model1 (LanguageModel): First model to compare\n",
        "        model2 (LanguageModel): Second model to compare\n",
        "        test_data (List[np.ndarray]): Test sequences\n",
        "        tokenizer: Tokenizer for decoding (optional, for display purposes)\n",
        "        context_length (int): Context length to consider\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results including overall perplexities and context comparisons\n",
        "    \"\"\"\n",
        "    print(\"Detailed Model Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Overall perplexity comparison\n",
        "    try:\n",
        "        ppl1 = model1.perplexity(test_data)\n",
        "        ppl2 = model2.perplexity(test_data)\n",
        "\n",
        "        model1_name = model1.__class__.__name__\n",
        "        model2_name = model2.__class__.__name__\n",
        "\n",
        "        print(f\"{model1_name} overall perplexity: {ppl1:.3f}\")\n",
        "        print(f\"{model2_name} overall perplexity: {ppl2:.3f}\")\n",
        "        print(f\"Better overall model: {model1_name if ppl1 < ppl2 else model2_name}\")\n",
        "        print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating overall perplexity: {e}\")\n",
        "        return {}\n",
        "\n",
        "    # Context-level analysis\n",
        "    context_comparisons = []\n",
        "    model1_better_contexts = []\n",
        "    model2_better_contexts = []\n",
        "\n",
        "    print(\"Analyzing context-level performance...\")\n",
        "\n",
        "    for seq_idx, sequence in enumerate(test_data):\n",
        "        for i in range(context_length, len(sequence)):\n",
        "            context = sequence[i - context_length:i]\n",
        "            target_token = sequence[i]\n",
        "\n",
        "            try:\n",
        "                # Get predictions from both models\n",
        "                probs1 = model1.get_next_token_probs(context)\n",
        "                probs2 = model2.get_next_token_probs(context)\n",
        "\n",
        "                # Calculate log probabilities for the actual target\n",
        "                prob1 = max(probs1[target_token], 1e-10)\n",
        "                prob2 = max(probs2[target_token], 1e-10)\n",
        "\n",
        "                log_prob1 = np.log2(prob1)\n",
        "                log_prob2 = np.log2(prob2)\n",
        "\n",
        "                # Store comparison data\n",
        "                context_info = {\n",
        "                    'context': context.copy(),\n",
        "                    'target': target_token,\n",
        "                    'log_prob1': log_prob1,\n",
        "                    'log_prob2': log_prob2,\n",
        "                    'seq_idx': seq_idx,\n",
        "                    'pos': i\n",
        "                }\n",
        "                context_comparisons.append(context_info)\n",
        "\n",
        "                # Categorize based on which model is better\n",
        "                if log_prob1 > log_prob2:  # Higher log prob = better\n",
        "                    model1_better_contexts.append(context_info)\n",
        "                else:\n",
        "                    model2_better_contexts.append(context_info)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing context at seq {seq_idx}, pos {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Calculate statistics\n",
        "    total_contexts = len(context_comparisons)\n",
        "    model1_wins = len(model1_better_contexts)\n",
        "    model2_wins = len(model2_better_contexts)\n",
        "\n",
        "    print(f\"Total contexts analyzed: {total_contexts}\")\n",
        "    print(f\"{model1_name} better contexts: {model1_wins} ({100*model1_wins/total_contexts:.1f}%)\")\n",
        "    print(f\"{model2_name} better contexts: {model2_wins} ({100*model2_wins/total_contexts:.1f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Find patterns in contexts where each model excels\n",
        "    def analyze_context_patterns(better_contexts, model_name, top_k=10):\n",
        "        print(f\"Top {top_k} unique contexts where {model_name} excels:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Group contexts by (context, target) pairs\n",
        "        context_groups = {}\n",
        "        for ctx_info in better_contexts:\n",
        "            context_tuple = tuple(ctx_info['context'])\n",
        "            target = ctx_info['target']\n",
        "            key = (context_tuple, target)\n",
        "\n",
        "            if key not in context_groups:\n",
        "                context_groups[key] = {\n",
        "                    'contexts': [],\n",
        "                    'best_diff': 0,\n",
        "                    'context': ctx_info['context'],\n",
        "                    'target': target\n",
        "                }\n",
        "\n",
        "            context_groups[key]['contexts'].append(ctx_info)\n",
        "\n",
        "            # Track the best performance difference for this context\n",
        "            diff = (ctx_info['log_prob1'] - ctx_info['log_prob2']\n",
        "                   if model_name == model1_name\n",
        "                   else ctx_info['log_prob2'] - ctx_info['log_prob1'])\n",
        "\n",
        "            if diff > context_groups[key]['best_diff']:\n",
        "                context_groups[key]['best_diff'] = diff\n",
        "\n",
        "        # Sort by best performance difference\n",
        "        sorted_groups = sorted(context_groups.values(),\n",
        "                             key=lambda x: x['best_diff'],\n",
        "                             reverse=True)\n",
        "\n",
        "        for i, group in enumerate(sorted_groups[:top_k]):\n",
        "            context = group['context']\n",
        "            target = group['target']\n",
        "            count = len(group['contexts'])\n",
        "            best_diff = group['best_diff']\n",
        "\n",
        "            # Format context display\n",
        "            if tokenizer is not None:\n",
        "                try:\n",
        "                    context_text = tokenizer.decode(context[-min(5, len(context)):])\n",
        "                    target_text = tokenizer.decode([target])\n",
        "                    print(f\"{i+1:2d}. Context: '{context_text}' → Target: '{target_text}' (×{count})\")\n",
        "                except:\n",
        "                    print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "            else:\n",
        "                print(f\"{i+1:2d}. Context: {context[-min(5, len(context)):]} → Target: {target} (×{count})\")\n",
        "\n",
        "            # Show best probability difference for this context type\n",
        "            print(f\"     Best log-prob difference: {best_diff:.3f}\")\n",
        "\n",
        "            # If there are multiple instances, show average difference\n",
        "            if count > 1:\n",
        "                avg_diff = sum((ctx['log_prob1'] - ctx['log_prob2']\n",
        "                              if model_name == model1_name\n",
        "                              else ctx['log_prob2'] - ctx['log_prob1'])\n",
        "                             for ctx in group['contexts']) / count\n",
        "                print(f\"     Average log-prob difference: {avg_diff:.3f}\")\n",
        "            print()\n",
        "\n",
        "    # Analyze patterns for both models\n",
        "    if model1_better_contexts:\n",
        "        analyze_context_patterns(model1_better_contexts, model1_name)\n",
        "\n",
        "    if model2_better_contexts:\n",
        "        analyze_context_patterns(model2_better_contexts, model2_name)\n",
        "\n",
        "    # Analyze context length effects\n",
        "    print(\"Performance by context length:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    context_length_stats = {}\n",
        "    for ctx_info in context_comparisons:\n",
        "        ctx_len = len(ctx_info['context'])\n",
        "        if ctx_len not in context_length_stats:\n",
        "            context_length_stats[ctx_len] = {'model1_wins': 0, 'model2_wins': 0, 'total': 0}\n",
        "\n",
        "        context_length_stats[ctx_len]['total'] += 1\n",
        "        if ctx_info['log_prob1'] > ctx_info['log_prob2']:\n",
        "            context_length_stats[ctx_len]['model1_wins'] += 1\n",
        "        else:\n",
        "            context_length_stats[ctx_len]['model2_wins'] += 1\n",
        "\n",
        "    for ctx_len in sorted(context_length_stats.keys()):\n",
        "        stats = context_length_stats[ctx_len]\n",
        "        model1_pct = 100 * stats['model1_wins'] / stats['total']\n",
        "        model2_pct = 100 * stats['model2_wins'] / stats['total']\n",
        "        print(f\"Length {ctx_len:2d}: {model1_name} {model1_pct:5.1f}% | {model2_name} {model2_pct:5.1f}% ({stats['total']} examples)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Return structured results\n",
        "    return {\n",
        "        'overall_perplexity': {model1_name: ppl1, model2_name: ppl2},\n",
        "        'context_level': {\n",
        "            'total_contexts': total_contexts,\n",
        "            f'{model1_name}_wins': model1_wins,\n",
        "            f'{model2_name}_wins': model2_wins,\n",
        "            f'{model1_name}_better_contexts': model1_better_contexts[:10],  # Top 10\n",
        "            f'{model2_name}_better_contexts': model2_better_contexts[:10],  # Top 10\n",
        "        },\n",
        "        'context_length_stats': context_length_stats\n",
        "    }"
      ],
      "metadata": {
        "id": "xMtmXsBVvcWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to comment out portions of the code and run it multiple times, or to\n",
        "# take it out of the main() function. If you're struggling to lower your\n",
        "# perplexity, you can play around with the model hyperparameters like the\n",
        "# learning rate, batch size, and number of epochs.\n",
        "\n",
        "def main():\n",
        "    # Here we're training on train.txt and evaluating on test.txt.\n",
        "    # However, you might find it useful to play with tiny.txt while you're debugging.\n",
        "    # If you're running into memory issues, you can try training on a smaller set of\n",
        "    # sentences by truncating train.txt, but you should always report your final\n",
        "    # results on test.txt (and think about ways of making your code more efficient)!\n",
        "\n",
        "    train_data, tokenizer = load_data(\"train.txt\", tokenizer=None) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded training data\")\n",
        "    test_data, _ = load_data(\"test.txt\", tokenizer=tokenizer) # Feel free to swap with tiny.txt for testing\n",
        "    print(\"Loaded test data\")\n",
        "\n",
        "    print('N-gram')\n",
        "    ngram_model = NGramModel(tokenizer.get_vocab_size(), n=3)\n",
        "    ngram_model.train(train_data)\n",
        "    print(tokenizer.decode(ngram_model.generate_text(test_data[0][:1])))\n",
        "    print(f\"Perplexity: {ngram_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('Log-linear')\n",
        "    # log_linear_model = LogLinearModel(tokenizer.get_vocab_size())\n",
        "    # log_linear_model.train(train_data)\n",
        "    # print(tokenizer.decode(log_linear_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {log_linear_model.perplexity(test_data)}\")\n",
        "\n",
        "    # print('CBOW')\n",
        "    # cbow_model = CBOWModel(tokenizer.get_vocab_size(), embedding_dim=100, context_size=3)\n",
        "    # cbow_model.train(train_data, epochs=10)\n",
        "    # print(tokenizer.decode(cbow_model.generate_text(test_data[0][:1])))\n",
        "    # print(f\"Perplexity: {cbow_model.perplexity(test_data)}\")\n",
        "\n",
        "    # evaluate_models([ngram_model, log_linear_model,cbow_model], test_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "MNOveioKceQH",
        "outputId": "329b849f-11e4-4e02-f97e-80eb9c3edad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "No such file or directory (os error 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3842151816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3842151816.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# sentences by truncating train.txt, but you should always report your final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# results on test.txt (and think about ways of making your code more efficient)!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded training data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1932212007.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filepath, tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWhitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#trainer = BpeTrainer(special_tokens=[\"[PAD]\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Read the text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
          ]
        }
      ]
    }
  ]
}